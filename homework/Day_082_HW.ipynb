{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Day082_HW.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ls0AzpzY0tto","colab_type":"text"},"source":["## Work\n","1. 請比較使用不同層數以及不同 Dropout rate 對訓練的效果\n","2. 將 optimizer 改成使用 Adam 並加上適當的 dropout rate 檢視結果"]},{"cell_type":"code","metadata":{"id":"WNY64-P70ttr","colab_type":"code","outputId":"7404f756-a124-4d8b-f5ba-7e296a932e2e","executionInfo":{"status":"ok","timestamp":1563684604559,"user_tz":-480,"elapsed":3673,"user":{"displayName":"陳冠樺Joi Chen","photoUrl":"","userId":"08996694790925091938"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import os\n","import keras\n","import itertools\n","# Disable GPU\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"vZ8_LHnH0ttz","colab_type":"code","colab":{}},"source":["train, test = keras.datasets.cifar10.load_data()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h_qTk9bO0tt4","colab_type":"code","colab":{}},"source":["## 資料前處理\n","def preproc_x(x, flatten=True):\n","    x = x / 255.\n","    if flatten:\n","        x = x.reshape((len(x), -1))\n","    return x\n","\n","def preproc_y(y, num_classes=10):\n","    if y.shape[-1] == 1:\n","        y = keras.utils.to_categorical(y, num_classes)\n","    return y    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BhTNXf0E0tt8","colab_type":"code","colab":{}},"source":["x_train, y_train = train\n","x_test, y_test = test\n","\n","# Preproc the inputs\n","x_train = preproc_x(x_train)\n","x_test = preproc_x(x_test)\n","\n","# Preprc the outputs\n","y_train = preproc_y(y_train)\n","y_test = preproc_y(y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zOaZDUJQ0tuE","colab_type":"code","colab":{}},"source":["from keras.layers import Dropout\n","\"\"\"\n","建立神經網路，並加入 dropout layer\n","\"\"\"\n","def build_mlp(input_shape, output_units=10, num_neurons=[512, 256, 128], drp_ratio=0.3):\n","    input_layer = keras.layers.Input(input_shape)\n","    \n","    for i, n_units in enumerate(num_neurons):\n","        if i == 0:\n","            x = keras.layers.Dense(units=n_units, \n","                                   activation=\"relu\", \n","                                   name=\"hidden_layer\"+str(i+1))(input_layer)\n","            x = Dropout(drp_ratio)(x)\n","        else:\n","            x = keras.layers.Dense(units=n_units, \n","                                   activation=\"relu\", \n","                                   name=\"hidden_layer\"+str(i+1))(x)\n","            x = Dropout(drp_ratio)(x)\n","    \n","    out = keras.layers.Dense(units=output_units, activation=\"softmax\", name=\"output\")(x)\n","    \n","    model = keras.models.Model(inputs=[input_layer], outputs=[out])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fq33RLoP0tuL","colab_type":"code","colab":{}},"source":["\"\"\"Code Here\n","設定超參數\n","\"\"\"\n","LEARNING_RATE = 1e-3\n","EPOCHS = 50\n","BATCH_SIZE = 256\n","MOMENTUM = 0.95\n","Dropout_EXP = [0.25, 0.3, 0.5, 0.8]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"XoeO--SO0tuT","colab_type":"code","outputId":"e2179c74-f9c3-461c-9e04-c2933f1a398b","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["results = {}\n","\"\"\"Code Here\n","撰寫你的訓練流程並將結果用 dictionary 紀錄\n","\"\"\"\n","for DE in Dropout_EXP:\n","    model = build_mlp(input_shape=x_train.shape[1:], drp_ratio=DE)\n","    model.summary()\n","    optimizer = keras.optimizers.Adam(lr=LEARNING_RATE)\n","    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n","    model.fit(x_train, y_train, \n","              epochs=EPOCHS, \n","              batch_size=BATCH_SIZE, \n","              validation_data=(x_test, y_test), \n","              shuffle=True)\n","    # Collect results\n","    train_loss = model.history.history[\"loss\"]\n","    valid_loss = model.history.history[\"val_loss\"]\n","    train_acc = model.history.history[\"acc\"]\n","    valid_acc = model.history.history[\"val_acc\"]\n","    \n","    exp_name_tag = \"exp-lr-%s\" % str(DE)\n","    results[exp_name_tag] = {'train-loss': train_loss,\n","                             'valid-loss': valid_loss,\n","                             'train-acc': train_acc,\n","                             'valid-acc': valid_acc}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0721 04:50:06.616071 139909555218304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0721 04:50:06.636545 139909555218304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0721 04:50:06.640466 139909555218304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","W0721 04:50:06.660486 139909555218304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","W0721 04:50:06.672000 139909555218304 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","W0721 04:50:06.785541 139909555218304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","W0721 04:50:06.796545 139909555218304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","W0721 04:50:06.905556 139909555218304 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 3072)              0         \n","_________________________________________________________________\n","hidden_layer1 (Dense)        (None, 512)               1573376   \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 512)               0         \n","_________________________________________________________________\n","hidden_layer2 (Dense)        (None, 256)               131328    \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","hidden_layer3 (Dense)        (None, 128)               32896     \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","output (Dense)               (None, 10)                1290      \n","=================================================================\n","Total params: 1,738,890\n","Trainable params: 1,738,890\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 50000 samples, validate on 10000 samples\n","Epoch 1/50\n","50000/50000 [==============================] - 15s 293us/step - loss: 2.0441 - acc: 0.2415 - val_loss: 1.8436 - val_acc: 0.3348\n","Epoch 2/50\n","50000/50000 [==============================] - 14s 284us/step - loss: 1.8741 - acc: 0.3178 - val_loss: 1.7738 - val_acc: 0.3637\n","Epoch 3/50\n","50000/50000 [==============================] - 14s 285us/step - loss: 1.8250 - acc: 0.3383 - val_loss: 1.7004 - val_acc: 0.3917\n","Epoch 4/50\n","50000/50000 [==============================] - 14s 286us/step - loss: 1.7895 - acc: 0.3508 - val_loss: 1.6745 - val_acc: 0.4071\n","Epoch 5/50\n","50000/50000 [==============================] - 14s 288us/step - loss: 1.7581 - acc: 0.3621 - val_loss: 1.6674 - val_acc: 0.4182\n","Epoch 6/50\n","50000/50000 [==============================] - 14s 288us/step - loss: 1.7315 - acc: 0.3756 - val_loss: 1.6394 - val_acc: 0.4130\n","Epoch 7/50\n","50000/50000 [==============================] - 14s 282us/step - loss: 1.7051 - acc: 0.3840 - val_loss: 1.6187 - val_acc: 0.4364\n","Epoch 8/50\n","50000/50000 [==============================] - 14s 279us/step - loss: 1.7016 - acc: 0.3855 - val_loss: 1.6088 - val_acc: 0.4301\n","Epoch 9/50\n","50000/50000 [==============================] - 14s 276us/step - loss: 1.6786 - acc: 0.3942 - val_loss: 1.5853 - val_acc: 0.4421\n","Epoch 10/50\n","50000/50000 [==============================] - 14s 276us/step - loss: 1.6737 - acc: 0.3940 - val_loss: 1.5920 - val_acc: 0.4429\n","Epoch 11/50\n","50000/50000 [==============================] - 14s 274us/step - loss: 1.6594 - acc: 0.4011 - val_loss: 1.5977 - val_acc: 0.4358\n","Epoch 12/50\n","50000/50000 [==============================] - 14s 274us/step - loss: 1.6463 - acc: 0.4052 - val_loss: 1.5802 - val_acc: 0.4430\n","Epoch 13/50\n","50000/50000 [==============================] - 14s 274us/step - loss: 1.6385 - acc: 0.4086 - val_loss: 1.5479 - val_acc: 0.4514\n","Epoch 14/50\n","50000/50000 [==============================] - 14s 274us/step - loss: 1.6299 - acc: 0.4128 - val_loss: 1.5674 - val_acc: 0.4441\n","Epoch 15/50\n","50000/50000 [==============================] - 14s 273us/step - loss: 1.6202 - acc: 0.4142 - val_loss: 1.5263 - val_acc: 0.4572\n","Epoch 16/50\n","50000/50000 [==============================] - 14s 274us/step - loss: 1.6097 - acc: 0.4194 - val_loss: 1.5371 - val_acc: 0.4547\n","Epoch 17/50\n","50000/50000 [==============================] - 14s 273us/step - loss: 1.6191 - acc: 0.4158 - val_loss: 1.5680 - val_acc: 0.4485\n","Epoch 18/50\n","50000/50000 [==============================] - 14s 279us/step - loss: 1.6013 - acc: 0.4233 - val_loss: 1.5285 - val_acc: 0.4587\n","Epoch 19/50\n","50000/50000 [==============================] - 15s 306us/step - loss: 1.5902 - acc: 0.4261 - val_loss: 1.5392 - val_acc: 0.4615\n","Epoch 20/50\n","50000/50000 [==============================] - 14s 288us/step - loss: 1.5888 - acc: 0.4266 - val_loss: 1.5229 - val_acc: 0.4644\n","Epoch 21/50\n","50000/50000 [==============================] - 14s 289us/step - loss: 1.5823 - acc: 0.4290 - val_loss: 1.5256 - val_acc: 0.4662\n","Epoch 22/50\n","50000/50000 [==============================] - 15s 290us/step - loss: 1.5753 - acc: 0.4320 - val_loss: 1.5063 - val_acc: 0.4654\n","Epoch 23/50\n","50000/50000 [==============================] - 16s 311us/step - loss: 1.5716 - acc: 0.4349 - val_loss: 1.5298 - val_acc: 0.4609\n","Epoch 24/50\n","50000/50000 [==============================] - 14s 286us/step - loss: 1.5722 - acc: 0.4344 - val_loss: 1.5138 - val_acc: 0.4657\n","Epoch 25/50\n","50000/50000 [==============================] - 14s 287us/step - loss: 1.5628 - acc: 0.4367 - val_loss: 1.5183 - val_acc: 0.4617\n","Epoch 26/50\n","50000/50000 [==============================] - 14s 286us/step - loss: 1.5586 - acc: 0.4401 - val_loss: 1.5093 - val_acc: 0.4657\n","Epoch 27/50\n"," 5632/50000 [==>...........................] - ETA: 11s - loss: 1.5563 - acc: 0.4402"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ATaHsUiP0tuW","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\"\"\"Code Here\n","將結果繪出\n","\"\"\"\n","color_bar = [\"r\", \"g\", \"b\", \"y\", \"m\", \"k\"]\n","\n","plt.figure(figsize=(8,6))\n","for i, cond in enumerate(results.keys()):\n","    plt.plot(range(len(results[cond]['train-loss'])),results[cond]['train-loss'], '-', label=cond, color=color_bar[i])\n","    plt.plot(range(len(results[cond]['valid-loss'])),results[cond]['valid-loss'], '--', label=cond, color=color_bar[i])\n","plt.title(\"Loss\")\n","plt.legend()\n","plt.show()\n","\n","plt.figure(figsize=(8,6))\n","for i, cond in enumerate(results.keys()):\n","    plt.plot(range(len(results[cond]['train-acc'])),results[cond]['train-acc'], '-', label=cond, color=color_bar[i])\n","    plt.plot(range(len(results[cond]['valid-acc'])),results[cond]['valid-acc'], '--', label=cond, color=color_bar[i])\n","plt.title(\"Accuracy\")\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rumB5tg1YpQb","colab_type":"text"},"source":["Layer=2"]},{"cell_type":"code","metadata":{"id":"A4K4yDCmYoiD","colab_type":"code","colab":{}},"source":["from keras.layers import Dropout\n","\"\"\"\n","建立神經網路，並加入 dropout layer\n","\"\"\"\n","def build_mlp(input_shape, output_units=10, num_neurons=[512, 256], drp_ratio=0.5):\n","    input_layer = keras.layers.Input(input_shape)\n","    \n","    for i, n_units in enumerate(num_neurons):\n","        if i == 0:\n","            x = keras.layers.Dense(units=n_units, \n","                                   activation=\"relu\", \n","                                   name=\"hidden_layer\"+str(i+1))(input_layer)\n","            x = Dropout(drp_ratio)(x)\n","        else:\n","            x = keras.layers.Dense(units=n_units, \n","                                   activation=\"relu\", \n","                                   name=\"hidden_layer\"+str(i+1))(x)\n","            x = Dropout(drp_ratio)(x)\n","    \n","    out = keras.layers.Dense(units=output_units, activation=\"softmax\", name=\"output\")(x)\n","    \n","    model = keras.models.Model(inputs=[input_layer], outputs=[out])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2jDzqCelYufK","colab_type":"code","colab":{}},"source":["LEARNING_RATE = 1e-3\n","EPOCHS = 50\n","BATCH_SIZE = 256\n","MOMENTUM = 0.95\n","Dropout_EXP = [0.25, 0.3, 0.5, 0.8]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZJAbeVpYxw8","colab_type":"code","colab":{}},"source":["results = {}\n","\"\"\"Code Here\n","撰寫你的訓練流程並將結果用 dictionary 紀錄\n","\"\"\"\n","for DE in Dropout_EXP:\n","    keras.backend.clear_session() # 把舊的 Graph 清掉\n","    print(\"Experiment with DE = %.6f\" % (DE))\n","    model = build_mlp(input_shape=x_train.shape[1:], drp_ratio=DE)\n","    model.summary()\n","    optimizer = keras.optimizers.Adam(lr=LEARNING_RATE)\n","    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n","    model.fit(x_train, y_train, \n","              epochs=EPOCHS, \n","              batch_size=BATCH_SIZE, \n","              validation_data=(x_test, y_test), \n","              shuffle=True)\n","    # Collect results\n","    train_loss = model.history.history[\"loss\"]\n","    valid_loss = model.history.history[\"val_loss\"]\n","    train_acc = model.history.history[\"acc\"]\n","    valid_acc = model.history.history[\"val_acc\"]\n","    \n","    exp_name_tag = \"exp-DE-%s\" % str(DE)\n","    results[exp_name_tag] = {'train-loss': train_loss,\n","                             'valid-loss': valid_loss,\n","                             'train-acc': train_acc,\n","                             'valid-acc': valid_acc}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T8EPciE8Y0w3","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","color_bar = [\"r\", \"g\", \"b\", \"y\", \"m\", \"k\"]\n","\n","plt.figure(figsize=(8,6))\n","for i, cond in enumerate(results.keys()):\n","    plt.plot(range(len(results[cond]['train-loss'])),results[cond]['train-loss'], '-', label=cond, color=color_bar[i])\n","    plt.plot(range(len(results[cond]['valid-loss'])),results[cond]['valid-loss'], '--', label=cond, color=color_bar[i])\n","plt.title(\"Loss\")\n","plt.legend()\n","plt.show()\n","\n","plt.figure(figsize=(8,6))\n","for i, cond in enumerate(results.keys()):\n","    plt.plot(range(len(results[cond]['train-acc'])),results[cond]['train-acc'], '-', label=cond, color=color_bar[i])\n","    plt.plot(range(len(results[cond]['valid-acc'])),results[cond]['valid-acc'], '--', label=cond, color=color_bar[i])\n","plt.title(\"Accuracy\")\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]}]}